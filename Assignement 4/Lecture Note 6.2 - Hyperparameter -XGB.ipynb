{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hyperparamter Tuning - XGBoost\n",
    "1. Import the customer churn data (I have already cleaned it)\n",
    "2. Split the data into test and train sets\n",
    "3. Build data matrices - as XGBoost uses DMatrix\n",
    "4. Find the logloss of the model with default parameters\n",
    "5. Tune the parameters\n",
    "6. Find the logloss of the model with tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "df = pd.DataFrame(X, columns=cancer.feature_names)\n",
    "df['target'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the dimension of the data\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the data into train and test datasets\n",
    "# test:train = 3:7\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGBoost uses an internal data structure DMatrix - which optimizes both memory effieciency and speed\n",
    "# Hence, rather than using pandas dataframe, we will use data matrix - DMatrix\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "dm_train = xgb.DMatrix(X_train, label=y_train)\n",
    "dm_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building Model\n",
    "\n",
    "Ideal case would include an exhaustive gridsearch on all the parameters. However, such an approach is computationally intensive. Hence, we will focus on few important parameters and tune them sequentially. Following are the parameters that we will tune in this process:\n",
    "1. max_depth\n",
    "2. min_child_weight\n",
    "3. subsample\n",
    "4. colsample_bytree\n",
    "5. eta\n",
    "6. num_boost_rounds\n",
    "7. early_stopping_rounds\n",
    "\n",
    "We will use logistic loss function to assess the accuracy of predictions, as this is a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-logloss:0.48345\n",
      "[1]\tTest-logloss:0.36545\n",
      "[2]\tTest-logloss:0.28797\n",
      "[3]\tTest-logloss:0.23170\n",
      "[4]\tTest-logloss:0.19662\n",
      "[5]\tTest-logloss:0.17002\n",
      "[6]\tTest-logloss:0.14550\n",
      "[7]\tTest-logloss:0.13175\n",
      "[8]\tTest-logloss:0.12037\n",
      "[9]\tTest-logloss:0.11088\n",
      "[10]\tTest-logloss:0.10224\n",
      "[11]\tTest-logloss:0.09588\n",
      "[12]\tTest-logloss:0.09096\n",
      "[13]\tTest-logloss:0.08960\n",
      "[14]\tTest-logloss:0.08519\n",
      "[15]\tTest-logloss:0.08142\n",
      "[16]\tTest-logloss:0.07903\n",
      "[17]\tTest-logloss:0.07691\n",
      "[18]\tTest-logloss:0.07538\n",
      "[19]\tTest-logloss:0.07396\n",
      "[20]\tTest-logloss:0.07265\n",
      "[21]\tTest-logloss:0.07262\n",
      "[22]\tTest-logloss:0.07007\n",
      "[23]\tTest-logloss:0.07007\n",
      "[24]\tTest-logloss:0.06937\n",
      "[25]\tTest-logloss:0.06889\n",
      "[26]\tTest-logloss:0.06774\n",
      "[27]\tTest-logloss:0.06804\n",
      "[28]\tTest-logloss:0.06711\n",
      "[29]\tTest-logloss:0.06791\n",
      "[30]\tTest-logloss:0.06578\n",
      "[31]\tTest-logloss:0.06600\n",
      "[32]\tTest-logloss:0.06535\n",
      "[33]\tTest-logloss:0.06362\n",
      "[34]\tTest-logloss:0.06389\n",
      "[35]\tTest-logloss:0.06352\n",
      "[36]\tTest-logloss:0.06433\n",
      "[37]\tTest-logloss:0.06281\n",
      "[38]\tTest-logloss:0.06395\n",
      "[39]\tTest-logloss:0.06350\n",
      "[40]\tTest-logloss:0.06333\n",
      "[41]\tTest-logloss:0.06198\n",
      "[42]\tTest-logloss:0.06194\n",
      "[43]\tTest-logloss:0.06189\n",
      "[44]\tTest-logloss:0.06171\n",
      "[45]\tTest-logloss:0.06201\n",
      "[46]\tTest-logloss:0.06123\n",
      "[47]\tTest-logloss:0.06081\n",
      "[48]\tTest-logloss:0.06150\n",
      "[49]\tTest-logloss:0.06158\n",
      "[50]\tTest-logloss:0.06128\n",
      "[51]\tTest-logloss:0.06068\n",
      "[52]\tTest-logloss:0.06099\n",
      "[53]\tTest-logloss:0.06030\n",
      "[54]\tTest-logloss:0.06090\n",
      "[55]\tTest-logloss:0.05997\n",
      "[56]\tTest-logloss:0.06047\n",
      "[57]\tTest-logloss:0.05991\n",
      "[58]\tTest-logloss:0.06025\n",
      "[59]\tTest-logloss:0.06001\n",
      "[60]\tTest-logloss:0.05944\n",
      "[61]\tTest-logloss:0.05998\n",
      "[62]\tTest-logloss:0.05931\n",
      "[63]\tTest-logloss:0.05964\n",
      "[64]\tTest-logloss:0.05943\n",
      "[65]\tTest-logloss:0.05892\n",
      "[66]\tTest-logloss:0.05944\n",
      "[67]\tTest-logloss:0.05884\n",
      "[68]\tTest-logloss:0.05941\n",
      "[69]\tTest-logloss:0.05901\n",
      "[70]\tTest-logloss:0.05849\n",
      "[71]\tTest-logloss:0.05835\n",
      "[72]\tTest-logloss:0.05867\n",
      "[73]\tTest-logloss:0.05810\n",
      "[74]\tTest-logloss:0.05856\n",
      "[75]\tTest-logloss:0.05808\n",
      "[76]\tTest-logloss:0.05862\n",
      "[77]\tTest-logloss:0.05838\n",
      "[78]\tTest-logloss:0.05760\n",
      "[79]\tTest-logloss:0.05742\n",
      "[80]\tTest-logloss:0.05797\n",
      "[81]\tTest-logloss:0.05771\n",
      "[82]\tTest-logloss:0.05815\n",
      "[83]\tTest-logloss:0.05742\n",
      "[84]\tTest-logloss:0.05710\n",
      "[85]\tTest-logloss:0.05695\n",
      "[86]\tTest-logloss:0.05749\n",
      "[87]\tTest-logloss:0.05721\n",
      "[88]\tTest-logloss:0.05677\n",
      "[89]\tTest-logloss:0.05725\n",
      "[90]\tTest-logloss:0.05753\n",
      "[91]\tTest-logloss:0.05740\n",
      "[92]\tTest-logloss:0.05715\n",
      "[93]\tTest-logloss:0.05757\n",
      "[94]\tTest-logloss:0.05687\n",
      "[95]\tTest-logloss:0.05668\n",
      "[96]\tTest-logloss:0.05719\n",
      "[97]\tTest-logloss:0.05677\n",
      "[98]\tTest-logloss:0.05656\n",
      "[99]\tTest-logloss:0.05643\n",
      "Best Logloss: 0.056 | Rounds: 100\n"
     ]
    }
   ],
   "source": [
    "# We will set num_boost_rounds to 100, early_stopping_rounds to 10, and objective to binary:logistic.\n",
    "# All the other values at this stage are default values.\n",
    "# We will tune our model by chaning the default values.\n",
    "\n",
    "params = {'max_depth':6, 'min_child_weight':1, 'eta':0.3, 'subsample':1, \n",
    "          'colsample_bytree':1, 'objective':'binary:logistic',}\n",
    "\n",
    "# We will use logloss function to evaluate the model's performance\n",
    "params['eval_metric'] = \"logloss\"\n",
    "\n",
    "xgmodel = xgb.train(params, dtrain = dm_train, num_boost_round = 100, evals = [(dm_test,\"Test\")], \n",
    "                    early_stopping_rounds = 10)\n",
    "\n",
    "print(\"Best Logloss: {:.3f} | Rounds: {}\".format(xgmodel.best_score,xgmodel.best_iteration+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here, we found that the tenth round gave the best result and the results did not improve in the next 10 rounds. Hence, the iteration stopped at round 19 and we did not reach the maximum number of boosting rounds (100). Finding a suitable evidence to stop the iterations is important. Stopping the iterations when results do not improve prevents overfittig and the inefficient utilization of resources. We will use cross validation to tune the parameters within the params dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters: max-depth and min_child_weight\n",
    "# I realized that the optimal values are in the following ranges through multiple iterations\n",
    "\n",
    "gridsearch_params = [(max_depth, min_child_weight)\n",
    "                    for max_depth in range(1,4)\n",
    "                    for min_child_weight in range(17,21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth: 1 | min_child_weight: 17 with Logloss: 0.199\n",
      "\n",
      "max_depth: 1 | min_child_weight: 18 with Logloss: 0.206\n",
      "\n",
      "max_depth: 1 | min_child_weight: 19 with Logloss: 0.211\n",
      "\n",
      "max_depth: 1 | min_child_weight: 20 with Logloss: 0.217\n",
      "\n",
      "max_depth: 2 | min_child_weight: 17 with Logloss: 0.195\n",
      "\n",
      "max_depth: 2 | min_child_weight: 18 with Logloss: 0.201\n",
      "\n",
      "max_depth: 2 | min_child_weight: 19 with Logloss: 0.204\n",
      "\n",
      "max_depth: 2 | min_child_weight: 20 with Logloss: 0.219\n",
      "\n",
      "max_depth: 3 | min_child_weight: 17 with Logloss: 0.195\n",
      "\n",
      "max_depth: 3 | min_child_weight: 18 with Logloss: 0.201\n",
      "\n",
      "max_depth: 3 | min_child_weight: 19 with Logloss: 0.204\n",
      "\n",
      "max_depth: 3 | min_child_weight: 20 with Logloss: 0.219\n",
      "\n",
      "Best Parameters: max_depth: 2 | min_child_weight: 17 with Logloss: 0.195\n"
     ]
    }
   ],
   "source": [
    "logloss_min = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    \n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    \n",
    "    xg_cvresults = xgb.cv(params, dtrain = dm_train, num_boost_round = 100,\n",
    "                      seed = 0, nfold=10, metrics = {'logloss'}, early_stopping_rounds = 10,)\n",
    "    \n",
    "    logloss_mean = xg_cvresults['test-logloss-mean'].min()\n",
    "    \n",
    "    print(\"max_depth: {} | min_child_weight: {} with Logloss: {:.3}\\n\".format(max_depth,min_child_weight,logloss_mean))\n",
    "    \n",
    "    if logloss_mean < logloss_min:\n",
    "        logloss_min = logloss_mean\n",
    "        best_params = (max_depth, min_child_weight)\n",
    "\n",
    "        \n",
    "print(\"Best Parameters: max_depth: {} | min_child_weight: {} with Logloss: {:.3f}\". format(best_params[0], \n",
    "                                                                                  best_params[1], logloss_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Updating the parameters with the best values: max_depth = 2 and min_child_weight = 19\n",
    "\n",
    "params['max_depth'] = 2\n",
    "params['min_child_weight'] = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters: subsample and colsample_bytree\n",
    "# I found that the optimal values are in the following ranges through multiple iterations\n",
    "\n",
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(7,11)]\n",
    "    for colsample in [i/10. for i in range(1,5)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample: 0.7 | colsample: 0.1 with Logloss: 0.279\n",
      "\n",
      "subsample: 0.7 | colsample: 0.2 with Logloss: 0.271\n",
      "\n",
      "subsample: 0.7 | colsample: 0.3 with Logloss: 0.262\n",
      "\n",
      "subsample: 0.7 | colsample: 0.4 with Logloss: 0.259\n",
      "\n",
      "subsample: 0.8 | colsample: 0.1 with Logloss: 0.256\n",
      "\n",
      "subsample: 0.8 | colsample: 0.2 with Logloss: 0.250\n",
      "\n",
      "subsample: 0.8 | colsample: 0.3 with Logloss: 0.235\n",
      "\n",
      "subsample: 0.8 | colsample: 0.4 with Logloss: 0.236\n",
      "\n",
      "subsample: 0.9 | colsample: 0.1 with Logloss: 0.228\n",
      "\n",
      "subsample: 0.9 | colsample: 0.2 with Logloss: 0.223\n",
      "\n",
      "subsample: 0.9 | colsample: 0.3 with Logloss: 0.212\n",
      "\n",
      "subsample: 0.9 | colsample: 0.4 with Logloss: 0.220\n",
      "\n",
      "subsample: 1.0 | colsample: 0.1 with Logloss: 0.215\n",
      "\n",
      "subsample: 1.0 | colsample: 0.2 with Logloss: 0.211\n",
      "\n",
      "subsample: 1.0 | colsample: 0.3 with Logloss: 0.210\n",
      "\n",
      "subsample: 1.0 | colsample: 0.4 with Logloss: 0.204\n",
      "\n",
      "Best Parameters: subsample: 1.0 | colsample: 0.4 with Logloss: 0.204\n"
     ]
    }
   ],
   "source": [
    "logloss_min = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for subsample, colsample in (gridsearch_params):\n",
    "    \n",
    "    params['subsample'] = subsample\n",
    "    params['colsample_bytree'] = colsample\n",
    "    \n",
    "    xg_cvresults = xgb.cv(params, dtrain = dm_train, num_boost_round = 100,\n",
    "                      seed = 0, nfold=10, metrics = {'logloss'}, early_stopping_rounds = 10,)\n",
    "    \n",
    "    logloss_mean = xg_cvresults['test-logloss-mean'].min()\n",
    "    \n",
    "    print(\"subsample: {} | colsample: {} with Logloss: {:.3f}\\n\".format(subsample,colsample,logloss_mean))\n",
    "    \n",
    "    if logloss_mean < logloss_min:\n",
    "        logloss_min = logloss_mean\n",
    "        best_params = (subsample, colsample)\n",
    "        \n",
    "print(\"Best Parameters: subsample: {} | colsample: {} with Logloss: {:.3f}\". format(best_params[0], \n",
    "                                                                           best_params[1], logloss_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Updating the parameters with the best values: subsample = 0.9 and colsample = 0.4\n",
    "\n",
    "params['subsample'] = 0.9\n",
    "params['colsample_bytree'] = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta: 0.3 with Logloss: 0.22\n",
      "\n",
      "eta: 0.2 with Logloss: 0.22\n",
      "\n",
      "eta: 0.1 with Logloss: 0.224\n",
      "\n",
      "eta: 0.05 with Logloss: 0.218\n",
      "\n",
      "eta: 0.01 with Logloss: 0.333\n",
      "\n",
      "eta: 0.005 with Logloss: 0.454\n",
      "\n",
      "Best Parameter: eta: 0.05 with Logloss: 0.218\n"
     ]
    }
   ],
   "source": [
    "# Parameter: eta\n",
    "\n",
    "logloss_min = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for eta in [0.3, 0.2, 0.1, 0.05, 0.01, 0.005]:\n",
    "    \n",
    "    params['eta'] = eta\n",
    "    \n",
    "    xg_cvresults = xgb.cv(params, dtrain = dm_train, num_boost_round = 100,\n",
    "                      seed = 0, nfold=10, metrics = {'logloss'}, early_stopping_rounds = 10,)\n",
    "    \n",
    "    logloss_mean = xg_cvresults['test-logloss-mean'].min()\n",
    "    print(\"eta: {} with Logloss: {:.3}\\n\".format(eta,logloss_mean))\n",
    "    \n",
    "    if logloss_mean < logloss_min:\n",
    "        logloss_min = logloss_mean\n",
    "        best_params = eta\n",
    "        \n",
    "print(\"Best Parameter: eta: {} with Logloss: {:.3f}\". format(best_params, logloss_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Updating the eta parameter with the best value\n",
    "\n",
    "params['eta'] = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting the optimum paramters\n",
    "\n",
    "params = {'colsample_bytree': 0.4,\n",
    "          'eta': 0.3,\n",
    "          'eval_metric': 'logloss',\n",
    "          'max_depth': 2,\n",
    "          'min_child_weight': 19,\n",
    "          'objective':'binary:logistic',\n",
    "          'subsample': 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-logloss:0.52934\n",
      "[1]\tTest-logloss:0.41815\n",
      "[2]\tTest-logloss:0.33513\n",
      "[3]\tTest-logloss:0.27513\n",
      "[4]\tTest-logloss:0.23717\n",
      "[5]\tTest-logloss:0.21494\n",
      "[6]\tTest-logloss:0.19976\n",
      "[7]\tTest-logloss:0.19125\n",
      "[8]\tTest-logloss:0.19040\n",
      "[9]\tTest-logloss:0.19004\n",
      "[10]\tTest-logloss:0.18961\n",
      "[11]\tTest-logloss:0.18915\n",
      "[12]\tTest-logloss:0.18874\n",
      "[13]\tTest-logloss:0.18877\n",
      "[14]\tTest-logloss:0.18936\n",
      "[15]\tTest-logloss:0.18900\n",
      "[16]\tTest-logloss:0.18865\n",
      "[17]\tTest-logloss:0.18906\n",
      "[18]\tTest-logloss:0.18901\n",
      "[19]\tTest-logloss:0.18903\n",
      "[20]\tTest-logloss:0.18905\n",
      "[21]\tTest-logloss:0.18893\n",
      "[22]\tTest-logloss:0.18923\n",
      "[23]\tTest-logloss:0.18906\n",
      "[24]\tTest-logloss:0.18903\n",
      "[25]\tTest-logloss:0.18894\n",
      "[26]\tTest-logloss:0.18904\n",
      "Best Logloss: 0.189 in 17 rounds\n"
     ]
    }
   ],
   "source": [
    "# Finding the optimal number of rounds for the model with new parameters\n",
    "\n",
    "xgmodel_tuned = xgb.train(params, dtrain = dm_train, \n",
    "                          num_boost_round=100, evals=[(dm_test,\"Test\")], early_stopping_rounds=10)\n",
    "\n",
    "\n",
    "print(\"Best Logloss: {:.3f} in {} rounds\". format(xgmodel_tuned.best_score, xgmodel_tuned.best_iteration+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "With the tuned parameters we would need 17 rounds to achieve the best result The improvement after parameter tuning is marginal in our case. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "md(\"With the tuned parameters we would need {} rounds to achieve the best result The improvement after parameter tuning is marginal in our case. \".format(xgmodel_tuned.best_iteration+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Logloss of our model decreased from 0.424 to 0.417 However, we were able to see how parameters can be tuned.\n",
    "\n",
    "Here we have used only a few combination of parameters. We can further improve the impact of tuning; however, doing so would be computationally more expensive. More combination of parameters and wider ranges of values for each of those paramaters would have to be tested."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
